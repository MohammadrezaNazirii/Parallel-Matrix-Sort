==PROF== Connected to process 12517 (/home/amir/Desktop/managed/sort_float)
==PROF== Profiling "sortRows(float *, int, int)" - 0: 0%....50%....100% - 9 passes
==PROF== Profiling "sortRows(float *, int, int)" - 1: 0%....50%....100% - 9 passes
==PROF== Profiling "sortRows(float *, int, int)" - 2: 0%....50%....100% - 9 passes
==PROF== Profiling "sortRows(float *, int, int)" - 3: 0%....50%....100% - 9 passes
==PROF== Profiling "sortColumns" - 4: 0%....50%....100% - 9 passes
==PROF== Profiling "sortColumns" - 5: 0%....50%....100% - 9 passes
==PROF== Profiling "sortColumns" - 6: 0%....50%....100% - 9 passes
==PROF== Profiling "sortColumns" - 7: 0%....50%....100% - 9 passes
==PROF== Profiling "sortRows(float *, int, int)" - 8: 0%....50%....100% - 9 passes
==PROF== Profiling "sortRows(float *, int, int)" - 9: 0%....50%....100% - 9 passes
==PROF== Profiling "sortRows(float *, int, int)" - 10: 0%....50%....100% - 9 passes
==PROF== Profiling "sortRows(float *, int, int)" - 11: 0%....50%....100% - 9 passes
==PROF== Profiling "sortColumns" - 12: 0%....50%....100% - 9 passes
==PROF== Profiling "sortColumns" - 13: 0%....50%....100% - 9 passes
==PROF== Profiling "sortColumns" - 14: 0%....50%....100% - 9 passes
==PROF== Profiling "sortColumns" - 15: 0%....50%....100% - 9 passes
==PROF== Profiling "sortRows(float *, int, int)" - 16: 0%....50%....100% - 9 passes
==PROF== Profiling "sortRows(float *, int, int)" - 17: 0%....50%....100% - 9 passes
==PROF== Profiling "sortRows(float *, int, int)" - 18: 0%....50%....100% - 9 passes
==PROF== Profiling "sortRows(float *, int, int)" - 19: 0%....50%....100% - 9 passes
==PROF== Profiling "sortColumns" - 20: 0%....50%....100% - 9 passes
==PROF== Profiling "sortColumns" - 21: 0%....50%....100% - 9 passes
==PROF== Profiling "sortColumns" - 22: 0%....50%....100% - 9 passes
==PROF== Profiling "sortColumns" - 23: 0%....50%....100% - 9 passes
==PROF== Profiling "sortRows(float *, int, int)" - 24: 0%....50%....100% - 9 passes
==PROF== Profiling "sortRows(float *, int, int)" - 25: 0%....50%....100% - 9 passes
==PROF== Profiling "sortRows(float *, int, int)" - 26: 0%....50%....100% - 9 passes
==PROF== Profiling "sortRows(float *, int, int)" - 27: 0%....50%....100% - 9 passes
==PROF== Profiling "sortColumns" - 28: 0%....50%....100% - 9 passes
==PROF== Profiling "sortColumns" - 29: 0%....50%....100% - 9 passes
==PROF== Profiling "sortColumns" - 30: 0%....50%....100% - 9 passes
==PROF== Profiling "sortColumns" - 31: 0%....50%....100% - 9 passes
4Row sorting time: 15189.615234 ms
Row sorting time: 7817.370605 ms
Row sorting time: 7768.806641 ms
Row sorting time: 7823.702148 ms
Column sorting time: 24606.050781 ms
Column sorting time: 11331.956055 ms
Column sorting time: 11338.400391 ms
Column sorting time: 11339.535156 ms
Row sorting time: 12714.270508 ms
Row sorting time: 7573.934082 ms
Row sorting time: 7556.862793 ms
Row sorting time: 7560.558594 ms
Column sorting time: 13191.137695 ms
Column sorting time: 11307.021484 ms
Column sorting time: 11321.765625 ms
Column sorting time: 11289.551758 ms
Row sorting time: 12597.182617 ms
Row sorting time: 7563.819824 ms
Row sorting time: 7546.141602 ms
Row sorting time: 7575.391113 ms
Column sorting time: 11744.249023 ms
Column sorting time: 11296.146484 ms
Column sorting time: 11318.222656 ms
Column sorting time: 11318.590820 ms
Row sorting time: 12424.933594 ms
Row sorting time: 7577.010254 ms
Row sorting time: 7556.286621 ms
Row sorting time: 7575.881836 ms
Column sorting time: 11409.420898 ms
Column sorting time: 11315.058594 ms
Column sorting time: 11297.484375 ms
Column sorting time: 11302.250000 ms
total: 341148.562500
==PROF== Disconnected from process 12517
[12517] sort_float@127.0.0.1
  sortRows(float *, int, int) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    3,179,063,311
    Memory Throughput                 %            37.48
    DRAM Throughput                   %             1.13
    Duration                          s             1.66
    L1/TEX Cache Throughput           %            56.97
    L2 Cache Throughput               %            37.48
    SM Active Cycles              cycle 1,979,799,209.38
    Compute (SM) Throughput           %             2.81
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.17
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle      149,110,008
    Total DRAM Elapsed Cycles        cycle   52,938,061,824
    Average L1 Active Cycles         cycle 1,979,799,209.38
    Total L1 Elapsed Cycles          cycle   76,290,646,744
    Average L2 Active Cycles         cycle 2,871,985,151.12
    Total L2 Elapsed Cycles          cycle   48,083,332,496
    Average SM Active Cycles         cycle 1,979,799,209.38
    Total SM Elapsed Cycles          cycle   76,290,646,744
    Average SMSP Active Cycles       cycle   989,843,479.54
    Total SMSP Elapsed Cycles        cycle  305,162,586,976
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.49%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.72% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.44%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.86% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.49%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.72% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortRows(float *, int, int) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    1,641,128,549
    Memory Throughput                 %            44.97
    DRAM Throughput                   %             0.01
    Duration                         ms           854.75
    L1/TEX Cache Throughput           %            70.37
    L2 Cache Throughput               %             4.34
    SM Active Cycles              cycle 1,048,853,221.75
    Compute (SM) Throughput           %             5.45
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          532,540
    Total DRAM Elapsed Cycles        cycle   27,328,227,328
    Average L1 Active Cycles         cycle 1,048,853,221.75
    Total L1 Elapsed Cycles          cycle   39,393,115,608
    Average L2 Active Cycles         cycle   991,430,952.06
    Total L2 Elapsed Cycles          cycle   24,821,958,288
    Average SM Active Cycles         cycle 1,048,853,221.75
    Total SM Elapsed Cycles          cycle   39,393,115,608
    Average SMSP Active Cycles       cycle   524,023,806.34
    Total SMSP Elapsed Cycles        cycle  157,572,462,432
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.06%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 36.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.73%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.07% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.06%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 36.09% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortRows(float *, int, int) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    1,630,874,016
    Memory Throughput                 %            45.25
    DRAM Throughput                   %             0.01
    Duration                         ms           849.41
    L1/TEX Cache Throughput           %            70.42
    L2 Cache Throughput               %             4.37
    SM Active Cycles              cycle 1,048,014,282.38
    Compute (SM) Throughput           %             5.49
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          541,192
    Total DRAM Elapsed Cycles        cycle   27,157,467,136
    Average L1 Active Cycles         cycle 1,048,014,282.38
    Total L1 Elapsed Cycles          cycle   39,141,177,672
    Average L2 Active Cycles         cycle   990,066,046.62
    Total L2 Elapsed Cycles          cycle   24,666,770,128
    Average SM Active Cycles         cycle 1,048,014,282.38
    Total SM Elapsed Cycles          cycle   39,141,177,672
    Average SMSP Active Cycles       cycle   522,828,074.05
    Total SMSP Elapsed Cycles        cycle  156,564,710,688
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 22.97%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 35.74% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.78%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 67.94% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.97%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 35.74% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortRows(float *, int, int) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          7.99
    SM Frequency                    Ghz          1.92
    Elapsed Cycles                cycle 1,641,324,106
    Memory Throughput                 %         44.96
    DRAM Throughput                   %          0.01
    Duration                         ms        854.86
    L1/TEX Cache Throughput           %         70.32
    L2 Cache Throughput               %          4.34
    SM Active Cycles              cycle 1,049,563,742
    Compute (SM) Throughput           %          5.45
    ----------------------- ----------- -------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ---------------
    Metric Name                Metric Unit    Metric Value
    -------------------------- ----------- ---------------
    Average DRAM Active Cycles       cycle         533,772
    Total DRAM Elapsed Cycles        cycle  27,331,483,648
    Average L1 Active Cycles         cycle   1,049,563,742
    Total L1 Elapsed Cycles          cycle  39,396,126,712
    Average L2 Active Cycles         cycle  982,958,316.19
    Total L2 Elapsed Cycles          cycle  24,824,882,384
    Average SM Active Cycles         cycle   1,049,563,742
    Total SM Elapsed Cycles          cycle  39,396,126,712
    Average SMSP Active Cycles       cycle  524,643,020.07
    Total SMSP Elapsed Cycles        cycle 157,584,506,848
    -------------------------- ----------- ---------------

    OPT   Est. Speedup: 23.05%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 36.06% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.75%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.04% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.05%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 36.06% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortColumns(float *, int, int, int *) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          7.99
    SM Frequency                    Ghz          1.92
    Elapsed Cycles                cycle 5,215,170,335
    Memory Throughput                 %          7.64
    DRAM Throughput                   %          0.34
    Duration                          s          2.72
    L1/TEX Cache Throughput           %         12.54
    L2 Cache Throughput               %          7.18
    SM Active Cycles              cycle 3,179,189,654
    Compute (SM) Throughput           %          3.65
    ----------------------- ----------- -------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle       72,849,596
    Total DRAM Elapsed Cycles        cycle   86,843,507,712
    Average L1 Active Cycles         cycle    3,179,189,654
    Total L1 Elapsed Cycles          cycle  125,162,018,624
    Average L2 Active Cycles         cycle 1,210,160,992.38
    Total L2 Elapsed Cycles          cycle   78,879,451,344
    Average SM Active Cycles         cycle    3,179,189,654
    Total SM Elapsed Cycles          cycle  125,162,018,624
    Average SMSP Active Cycles       cycle 1,588,996,004.78
    Total SMSP Elapsed Cycles        cycle  500,648,074,496
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.8%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 39.05% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.19%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.53% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.8%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 39.05% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.12%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 73.80% above the average, while the minimum instance value is 19.02% below the      
          average.                                                                                                      

  sortColumns(float *, int, int, int *) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,385,911,959
    Memory Throughput                 %             1.84
    DRAM Throughput                   %             0.01
    Duration                          s             1.24
    L1/TEX Cache Throughput           %             2.95
    L2 Cache Throughput               %             1.47
    SM Active Cycles              cycle 1,491,667,090.21
    Compute (SM) Throughput           %             1.18
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          650,312
    Total DRAM Elapsed Cycles        cycle   39,730,432,000
    Average L1 Active Cycles         cycle 1,491,667,090.21
    Total L1 Elapsed Cycles          cycle   57,275,639,080
    Average L2 Active Cycles         cycle   521,099,655.38
    Total L2 Elapsed Cycles          cycle   36,055,650,240
    Average SM Active Cycles         cycle 1,491,667,090.21
    Total SM Elapsed Cycles          cycle   57,275,639,080
    Average SMSP Active Cycles       cycle   745,293,565.45
    Total SMSP Elapsed Cycles        cycle  229,102,556,320
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.43%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.48% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.48%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.77% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.43%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.48% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortColumns(float *, int, int, int *) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,386,190,570
    Memory Throughput                 %             1.84
    DRAM Throughput                   %             0.01
    Duration                          s             1.24
    L1/TEX Cache Throughput           %             2.95
    L2 Cache Throughput               %             1.47
    SM Active Cycles              cycle 1,491,363,674.54
    Compute (SM) Throughput           %             1.18
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          624,516
    Total DRAM Elapsed Cycles        cycle   39,735,070,720
    Average L1 Active Cycles         cycle 1,491,363,674.54
    Total L1 Elapsed Cycles          cycle   57,269,680,288
    Average L2 Active Cycles         cycle   521,106,569.62
    Total L2 Elapsed Cycles          cycle   36,091,132,160
    Average SM Active Cycles         cycle 1,491,363,674.54
    Total SM Elapsed Cycles          cycle   57,269,680,288
    Average SMSP Active Cycles       cycle   745,149,440.95
    Total SMSP Elapsed Cycles        cycle  229,078,721,152
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.44%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.50% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.47%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.77% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.44%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.50% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortColumns(float *, int, int, int *) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,386,598,474
    Memory Throughput                 %             1.85
    DRAM Throughput                   %             0.01
    Duration                          s             1.24
    L1/TEX Cache Throughput           %             2.95
    L2 Cache Throughput               %             1.47
    SM Active Cycles              cycle 1,491,532,735.62
    Compute (SM) Throughput           %             1.18
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          627,540
    Total DRAM Elapsed Cycles        cycle   39,741,861,888
    Average L1 Active Cycles         cycle 1,491,532,735.62
    Total L1 Elapsed Cycles          cycle   57,267,422,936
    Average L2 Active Cycles         cycle   520,861,283.50
    Total L2 Elapsed Cycles          cycle   36,097,301,664
    Average SM Active Cycles         cycle 1,491,532,735.62
    Total SM Elapsed Cycles          cycle   57,267,422,936
    Average SMSP Active Cycles       cycle   745,229,796.34
    Total SMSP Elapsed Cycles        cycle  229,069,691,744
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.43%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.49% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.48%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.76% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.43%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.49% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortRows(float *, int, int) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,684,197,432
    Memory Throughput                 %            38.97
    DRAM Throughput                   %             0.04
    Duration                          s             1.40
    L1/TEX Cache Throughput           %            63.03
    L2 Cache Throughput               %            37.28
    SM Active Cycles              cycle 1,648,978,872.21
    Compute (SM) Throughput           %             3.35
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle        4,540,728
    Total DRAM Elapsed Cycles        cycle   44,697,507,840
    Average L1 Active Cycles         cycle 1,648,978,872.21
    Total L1 Elapsed Cycles          cycle   64,015,213,032
    Average L2 Active Cycles         cycle 2,376,506,553.31
    Total L2 Elapsed Cycles          cycle   40,598,405,248
    Average SM Active Cycles         cycle 1,648,978,872.21
    Total SM Elapsed Cycles          cycle   64,015,213,032
    Average SMSP Active Cycles       cycle   823,697,954.74
    Total SMSP Elapsed Cycles        cycle  256,060,852,128
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.88%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 38.63% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.35%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.14% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.88%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 38.63% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortRows(float *, int, int) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    1,586,652,986
    Memory Throughput                 %            46.50
    DRAM Throughput                   %             0.01
    Duration                         ms           826.38
    L1/TEX Cache Throughput           %            70.53
    L2 Cache Throughput               %             4.47
    SM Active Cycles              cycle 1,046,409,942.83
    Compute (SM) Throughput           %             5.64
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.17
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          540,428
    Total DRAM Elapsed Cycles        cycle   26,421,094,400
    Average L1 Active Cycles         cycle 1,046,409,942.83
    Total L1 Elapsed Cycles          cycle   38,094,022,680
    Average L2 Active Cycles         cycle      989,047,502
    Total L2 Elapsed Cycles          cycle   23,998,036,240
    Average SM Active Cycles         cycle 1,046,409,942.83
    Total SM Elapsed Cycles          cycle   38,094,022,680
    Average SMSP Active Cycles       cycle   523,161,239.46
    Total SMSP Elapsed Cycles        cycle  152,376,090,720
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 22.45%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 34.06% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.1%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 67.04% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.45%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 34.06% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortRows(float *, int, int) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          7.99
    SM Frequency                    Ghz          1.92
    Elapsed Cycles                cycle 1,581,708,281
    Memory Throughput                 %         46.66
    DRAM Throughput                   %          0.01
    Duration                         ms        823.81
    L1/TEX Cache Throughput           %         70.64
    L2 Cache Throughput               %          4.49
    SM Active Cycles              cycle 1,044,794,008
    Compute (SM) Throughput           %          5.66
    ----------------------- ----------- -------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.17
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ---------------
    Metric Name                Metric Unit    Metric Value
    -------------------------- ----------- ---------------
    Average DRAM Active Cycles       cycle         533,824
    Total DRAM Elapsed Cycles        cycle  26,338,754,560
    Average L1 Active Cycles         cycle   1,044,794,008
    Total L1 Elapsed Cycles          cycle  37,963,281,472
    Average L2 Active Cycles         cycle  987,970,673.06
    Total L2 Elapsed Cycles          cycle  23,923,230,432
    Average SM Active Cycles         cycle   1,044,794,008
    Total SM Elapsed Cycles          cycle  37,963,281,472
    Average SMSP Active Cycles       cycle  523,003,511.91
    Total SMSP Elapsed Cycles        cycle 151,853,125,888
    -------------------------- ----------- ---------------

    OPT   Est. Speedup: 22.42%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 33.95% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.16%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 67.01% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.42%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 33.95% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortRows(float *, int, int) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    1,586,344,431
    Memory Throughput                 %            46.51
    DRAM Throughput                   %             0.01
    Duration                         ms           826.22
    L1/TEX Cache Throughput           %            70.45
    L2 Cache Throughput               %             4.47
    SM Active Cycles              cycle 1,047,677,759.58
    Compute (SM) Throughput           %             5.64
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          532,940
    Total DRAM Elapsed Cycles        cycle   26,415,958,016
    Average L1 Active Cycles         cycle 1,047,677,759.58
    Total L1 Elapsed Cycles          cycle   38,088,861,808
    Average L2 Active Cycles         cycle   981,452,417.62
    Total L2 Elapsed Cycles          cycle   23,993,353,472
    Average SM Active Cycles         cycle 1,047,677,759.58
    Total SM Elapsed Cycles          cycle   38,088,861,808
    Average SMSP Active Cycles       cycle   522,902,481.38
    Total SMSP Elapsed Cycles        cycle  152,355,447,232
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 22.43%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 33.98% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.09%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 67.04% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.43%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 33.98% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortColumns(float *, int, int, int *) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,782,994,963
    Memory Throughput                 %             3.08
    DRAM Throughput                   %             0.23
    Duration                          s             1.45
    L1/TEX Cache Throughput           %             4.95
    L2 Cache Throughput               %             3.01
    SM Active Cycles              cycle 1,729,639,351.33
    Compute (SM) Throughput           %             1.29
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle       27,138,808
    Total DRAM Elapsed Cycles        cycle   46,342,692,864
    Average L1 Active Cycles         cycle 1,729,639,351.33
    Total L1 Elapsed Cycles          cycle   66,837,351,752
    Average L2 Active Cycles         cycle   645,515,854.06
    Total L2 Elapsed Cycles          cycle   42,060,732,864
    Average SM Active Cycles         cycle 1,729,639,351.33
    Total SM Elapsed Cycles          cycle   66,837,351,752
    Average SMSP Active Cycles       cycle   864,493,112.99
    Total SMSP Elapsed Cycles        cycle  267,349,407,008
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.54%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.90% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.4%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.95% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.54%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.90% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.25%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 49.90% above the average, while the minimum instance value is 6.72% below the       
          average.                                                                                                      

  sortColumns(float *, int, int, int *) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,372,099,490
    Memory Throughput                 %             1.83
    DRAM Throughput                   %             0.01
    Duration                          s             1.24
    L1/TEX Cache Throughput           %             2.93
    L2 Cache Throughput               %             1.46
    SM Active Cycles              cycle 1,489,344,497.58
    Compute (SM) Throughput           %             1.17
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          533,308
    Total DRAM Elapsed Cycles        cycle   39,500,423,168
    Average L1 Active Cycles         cycle 1,489,344,497.58
    Total L1 Elapsed Cycles          cycle   57,196,291,968
    Average L2 Active Cycles         cycle   508,494,077.81
    Total L2 Elapsed Cycles          cycle   35,878,004,688
    Average SM Active Cycles         cycle 1,489,344,497.58
    Total SM Elapsed Cycles          cycle   57,196,291,968
    Average SMSP Active Cycles       cycle   744,298,022.51
    Total SMSP Elapsed Cycles        cycle  228,785,167,872
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.47%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.55% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.48%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.79% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.47%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.55% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortColumns(float *, int, int, int *) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,384,932,295
    Memory Throughput                 %             1.83
    DRAM Throughput                   %             0.01
    Duration                          s             1.24
    L1/TEX Cache Throughput           %             2.93
    L2 Cache Throughput               %             1.46
    SM Active Cycles              cycle 1,489,651,391.62
    Compute (SM) Throughput           %             1.17
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          540,532
    Total DRAM Elapsed Cycles        cycle   39,714,117,632
    Average L1 Active Cycles         cycle 1,489,651,391.62
    Total L1 Elapsed Cycles          cycle   57,238,969,584
    Average L2 Active Cycles         cycle   500,100,233.31
    Total L2 Elapsed Cycles          cycle   36,072,100,928
    Average SM Active Cycles         cycle 1,489,651,391.62
    Total SM Elapsed Cycles          cycle   57,238,969,584
    Average SMSP Active Cycles       cycle   744,108,642.41
    Total SMSP Elapsed Cycles        cycle  228,955,878,336
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.45%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.55% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.47%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.80% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.45%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.55% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortColumns(float *, int, int, int *) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,385,759,866
    Memory Throughput                 %             1.84
    DRAM Throughput                   %             0.01
    Duration                          s             1.24
    L1/TEX Cache Throughput           %             2.93
    L2 Cache Throughput               %             1.46
    SM Active Cycles              cycle 1,489,744,953.79
    Compute (SM) Throughput           %             1.18
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.17
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          532,908
    Total DRAM Elapsed Cycles        cycle   39,727,898,624
    Average L1 Active Cycles         cycle 1,489,744,953.79
    Total L1 Elapsed Cycles          cycle   57,100,529,008
    Average L2 Active Cycles         cycle   486,855,488.44
    Total L2 Elapsed Cycles          cycle   36,065,937,072
    Average SM Active Cycles         cycle 1,489,744,953.79
    Total SM Elapsed Cycles          cycle   57,100,529,008
    Average SMSP Active Cycles       cycle   743,891,878.61
    Total SMSP Elapsed Cycles        cycle  228,402,116,032
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.52%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.56% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.47%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.66% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.52%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.56% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortRows(float *, int, int) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,655,374,335
    Memory Throughput                 %            39.04
    DRAM Throughput                   %             0.04
    Duration                          s             1.38
    L1/TEX Cache Throughput           %            63.00
    L2 Cache Throughput               %            37.33
    SM Active Cycles              cycle 1,646,942,115.42
    Compute (SM) Throughput           %             3.37
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.15
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle        4,524,808
    Total DRAM Elapsed Cycles        cycle   44,217,542,656
    Average L1 Active Cycles         cycle 1,646,942,115.42
    Total L1 Elapsed Cycles          cycle   63,785,426,200
    Average L2 Active Cycles         cycle    2,356,926,442
    Total L2 Elapsed Cycles          cycle   40,162,488,192
    Average SM Active Cycles         cycle 1,646,942,115.42
    Total SM Elapsed Cycles          cycle   63,785,426,200
    Average SMSP Active Cycles       cycle   820,924,281.59
    Total SMSP Elapsed Cycles        cycle  255,141,704,800
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.6%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 38.08% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.32%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 69.03% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.6%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 38.08% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortRows(float *, int, int) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    1,586,853,741
    Memory Throughput                 %            46.51
    DRAM Throughput                   %             0.01
    Duration                         ms           826.49
    L1/TEX Cache Throughput           %            70.51
    L2 Cache Throughput               %             4.47
    SM Active Cycles              cycle 1,046,803,240.79
    Compute (SM) Throughput           %             5.64
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.17
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          531,336
    Total DRAM Elapsed Cycles        cycle   26,424,438,784
    Average L1 Active Cycles         cycle 1,046,803,240.79
    Total L1 Elapsed Cycles          cycle   38,081,786,168
    Average L2 Active Cycles         cycle   988,615,000.50
    Total L2 Elapsed Cycles          cycle   24,001,083,792
    Average SM Active Cycles         cycle 1,046,803,240.79
    Total SM Elapsed Cycles          cycle   38,081,786,168
    Average SMSP Active Cycles       cycle   523,028,973.67
    Total SMSP Elapsed Cycles        cycle  152,327,144,672
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 22.45%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 34.03% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.1%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 67.05% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.45%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 34.03% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortRows(float *, int, int) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    1,582,273,895
    Memory Throughput                 %            46.64
    DRAM Throughput                   %             0.01
    Duration                         ms           824.10
    L1/TEX Cache Throughput           %            70.62
    L2 Cache Throughput               %             4.49
    SM Active Cycles              cycle 1,045,134,304.33
    Compute (SM) Throughput           %             5.65
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.17
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          538,108
    Total DRAM Elapsed Cycles        cycle   26,348,173,312
    Average L1 Active Cycles         cycle 1,045,134,304.33
    Total L1 Elapsed Cycles          cycle   37,978,701,840
    Average L2 Active Cycles         cycle   987,642,499.38
    Total L2 Elapsed Cycles          cycle   23,931,817,184
    Average SM Active Cycles         cycle 1,045,134,304.33
    Total SM Elapsed Cycles          cycle   37,978,701,840
    Average SMSP Active Cycles       cycle   522,463,745.65
    Total SMSP Elapsed Cycles        cycle  151,914,807,360
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 22.4%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 33.92% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.11%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 66.96% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.4%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 33.92% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortRows(float *, int, int) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    1,587,120,233
    Memory Throughput                 %            46.51
    DRAM Throughput                   %             0.01
    Duration                         ms           826.63
    L1/TEX Cache Throughput           %            70.56
    L2 Cache Throughput               %             4.47
    SM Active Cycles              cycle 1,046,054,954.46
    Compute (SM) Throughput           %             5.64
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.18
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          541,188
    Total DRAM Elapsed Cycles        cycle   26,428,876,800
    Average L1 Active Cycles         cycle 1,046,054,954.46
    Total L1 Elapsed Cycles          cycle   38,088,442,880
    Average L2 Active Cycles         cycle   981,255,429.75
    Total L2 Elapsed Cycles          cycle   24,005,134,160
    Average SM Active Cycles         cycle 1,046,054,954.46
    Total SM Elapsed Cycles          cycle   38,088,442,880
    Average SMSP Active Cycles       cycle   523,138,356.47
    Total SMSP Elapsed Cycles        cycle  152,353,771,520
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 22.46%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 34.08% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.09%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 67.03% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.46%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 34.08% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortColumns(float *, int, int, int *) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,475,628,268
    Memory Throughput                 %             2.08
    DRAM Throughput                   %             0.08
    Duration                          s             1.29
    L1/TEX Cache Throughput           %             3.34
    L2 Cache Throughput               %             1.72
    SM Active Cycles              cycle 1,545,094,259.21
    Compute (SM) Throughput           %             1.21
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.17
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle        8,089,848
    Total DRAM Elapsed Cycles        cycle   41,224,396,288
    Average L1 Active Cycles         cycle 1,545,094,259.21
    Total L1 Elapsed Cycles          cycle   59,430,154,400
    Average L2 Active Cycles         cycle   542,518,924.12
    Total L2 Elapsed Cycles          cycle   37,443,877,232
    Average SM Active Cycles         cycle 1,545,094,259.21
    Total SM Elapsed Cycles          cycle   59,430,154,400
    Average SMSP Active Cycles       cycle   772,247,103.07
    Total SMSP Elapsed Cycles        cycle  237,720,617,600
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.49%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.65% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.41%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.67% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.49%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.65% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.13%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.13% above the average, while the minimum instance value is 1.97% below the       
          average.                                                                                                      

  sortColumns(float *, int, int, int *) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,368,975,094
    Memory Throughput                 %             1.83
    DRAM Throughput                   %             0.01
    Duration                          s             1.23
    L1/TEX Cache Throughput           %             2.93
    L2 Cache Throughput               %             1.46
    SM Active Cycles              cycle 1,488,982,480.92
    Compute (SM) Throughput           %             1.17
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          539,764
    Total DRAM Elapsed Cycles        cycle   39,448,395,776
    Average L1 Active Cycles         cycle 1,488,982,480.92
    Total L1 Elapsed Cycles          cycle   57,240,820,752
    Average L2 Active Cycles         cycle      496,715,661
    Total L2 Elapsed Cycles          cycle   35,830,748,048
    Average SM Active Cycles         cycle 1,488,982,480.92
    Total SM Elapsed Cycles          cycle   57,240,820,752
    Average SMSP Active Cycles       cycle   743,895,207.97
    Total SMSP Elapsed Cycles        cycle  228,963,283,008
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.42%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.51% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.46%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.81% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.42%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.51% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortColumns(float *, int, int, int *) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,385,347,517
    Memory Throughput                 %             1.83
    DRAM Throughput                   %             0.01
    Duration                          s             1.24
    L1/TEX Cache Throughput           %             2.93
    L2 Cache Throughput               %             1.45
    SM Active Cycles              cycle 1,490,293,864.42
    Compute (SM) Throughput           %             1.17
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          540,984
    Total DRAM Elapsed Cycles        cycle   39,721,031,680
    Average L1 Active Cycles         cycle 1,490,293,864.42
    Total L1 Elapsed Cycles          cycle   57,238,229,640
    Average L2 Active Cycles         cycle   505,909,755.06
    Total L2 Elapsed Cycles          cycle   36,078,380,800
    Average SM Active Cycles         cycle 1,490,293,864.42
    Total SM Elapsed Cycles          cycle   57,238,229,640
    Average SMSP Active Cycles       cycle   744,314,439.49
    Total SMSP Elapsed Cycles        cycle  228,952,918,560
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.43%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.50% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.47%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.80% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.43%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.50% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortColumns(float *, int, int, int *) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,383,252,311
    Memory Throughput                 %             1.83
    DRAM Throughput                   %             0.01
    Duration                          s             1.24
    L1/TEX Cache Throughput           %             2.93
    L2 Cache Throughput               %             1.46
    SM Active Cycles              cycle 1,488,889,208.17
    Compute (SM) Throughput           %             1.17
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.17
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          532,796
    Total DRAM Elapsed Cycles        cycle   39,686,141,952
    Average L1 Active Cycles         cycle 1,488,889,208.17
    Total L1 Elapsed Cycles          cycle   57,256,698,560
    Average L2 Active Cycles         cycle   497,829,501.56
    Total L2 Elapsed Cycles          cycle   36,027,494,320
    Average SM Active Cycles         cycle 1,488,889,208.17
    Total SM Elapsed Cycles          cycle   57,256,698,560
    Average SMSP Active Cycles       cycle   744,235,456.25
    Total SMSP Elapsed Cycles        cycle  229,026,794,240
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.46%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.58% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.46%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.78% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.46%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.58% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortRows(float *, int, int) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,618,513,437
    Memory Throughput                 %            39.22
    DRAM Throughput                   %             0.04
    Duration                          s             1.36
    L1/TEX Cache Throughput           %            62.75
    L2 Cache Throughput               %            36.43
    SM Active Cycles              cycle 1,636,478,973.58
    Compute (SM) Throughput           %             3.42
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.15
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle        4,028,100
    Total DRAM Elapsed Cycles        cycle   43,603,732,480
    Average L1 Active Cycles         cycle 1,636,478,973.58
    Total L1 Elapsed Cycles          cycle   62,834,543,152
    Average L2 Active Cycles         cycle 2,312,264,567.88
    Total L2 Elapsed Cycles          cycle   39,604,966,864
    Average SM Active Cycles         cycle 1,636,478,973.58
    Total SM Elapsed Cycles          cycle   62,834,543,152
    Average SMSP Active Cycles       cycle   816,074,432.04
    Total SMSP Elapsed Cycles        cycle  251,338,172,608
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.38%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.40% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.47%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.86% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.38%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.40% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortRows(float *, int, int) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    1,591,219,640
    Memory Throughput                 %            46.49
    DRAM Throughput                   %             0.01
    Duration                         ms           828.76
    L1/TEX Cache Throughput           %            70.47
    L2 Cache Throughput               %             4.46
    SM Active Cycles              cycle 1,047,370,138.75
    Compute (SM) Throughput           %             5.63
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          535,992
    Total DRAM Elapsed Cycles        cycle   26,497,139,712
    Average L1 Active Cycles         cycle 1,047,370,138.75
    Total L1 Elapsed Cycles          cycle   38,102,234,472
    Average L2 Active Cycles         cycle   989,083,483.31
    Total L2 Elapsed Cycles          cycle   24,067,141,440
    Average SM Active Cycles         cycle 1,047,370,138.75
    Total SM Elapsed Cycles          cycle   38,102,234,472
    Average SMSP Active Cycles       cycle   523,238,229.99
    Total SMSP Elapsed Cycles        cycle  152,408,937,888
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 22.42%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 33.98% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.09%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 67.03% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.42%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 33.98% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortRows(float *, int, int) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    1,582,165,197
    Memory Throughput                 %            46.62
    DRAM Throughput                   %             0.01
    Duration                         ms           824.04
    L1/TEX Cache Throughput           %            70.54
    L2 Cache Throughput               %             4.49
    SM Active Cycles              cycle 1,046,252,509.29
    Compute (SM) Throughput           %             5.65
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          537,912
    Total DRAM Elapsed Cycles        cycle   26,346,364,928
    Average L1 Active Cycles         cycle 1,046,252,509.29
    Total L1 Elapsed Cycles          cycle   37,993,631,632
    Average L2 Active Cycles         cycle      987,597,883
    Total L2 Elapsed Cycles          cycle   23,930,173,152
    Average SM Active Cycles         cycle 1,046,252,509.29
    Total SM Elapsed Cycles          cycle   37,993,631,632
    Average SMSP Active Cycles       cycle   522,370,264.39
    Total SMSP Elapsed Cycles        cycle  151,974,526,528
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 22.37%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 33.85% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.1%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 66.98% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.37%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 33.85% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortRows(float *, int, int) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    1,586,303,277
    Memory Throughput                 %            46.52
    DRAM Throughput                   %             0.01
    Duration                         ms           826.20
    L1/TEX Cache Throughput           %            70.45
    L2 Cache Throughput               %             4.48
    SM Active Cycles              cycle 1,047,595,449.88
    Compute (SM) Throughput           %             5.64
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          541,128
    Total DRAM Elapsed Cycles        cycle   26,415,270,912
    Average L1 Active Cycles         cycle 1,047,595,449.88
    Total L1 Elapsed Cycles          cycle   38,078,290,088
    Average L2 Active Cycles         cycle   980,741,924.06
    Total L2 Elapsed Cycles          cycle   23,972,489,568
    Average SM Active Cycles         cycle 1,047,595,449.88
    Total SM Elapsed Cycles          cycle   38,078,290,088
    Average SMSP Active Cycles       cycle   523,166,174.03
    Total SMSP Elapsed Cycles        cycle  152,313,160,352
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 22.45%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 34.00% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.1%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 67.03% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.45%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 34.00% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortColumns(float *, int, int, int *) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,403,940,907
    Memory Throughput                 %             1.88
    DRAM Throughput                   %             0.04
    Duration                          s             1.25
    L1/TEX Cache Throughput           %             3.01
    L2 Cache Throughput               %             1.51
    SM Active Cycles              cycle 1,501,810,574.42
    Compute (SM) Throughput           %             1.18
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle        3,533,456
    Total DRAM Elapsed Cycles        cycle   40,030,650,368
    Average L1 Active Cycles         cycle 1,501,810,574.42
    Total L1 Elapsed Cycles          cycle   57,686,008,680
    Average L2 Active Cycles         cycle   526,478,612.69
    Total L2 Elapsed Cycles          cycle   36,342,463,392
    Average SM Active Cycles         cycle 1,501,810,574.42
    Total SM Elapsed Cycles          cycle   57,686,008,680
    Average SMSP Active Cycles       cycle   750,311,146.88
    Total SMSP Elapsed Cycles        cycle  230,744,034,720
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.45%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.53% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.47%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.78% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.45%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.53% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortColumns(float *, int, int, int *) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,384,620,066
    Memory Throughput                 %             1.84
    DRAM Throughput                   %             0.01
    Duration                          s             1.24
    L1/TEX Cache Throughput           %             2.93
    L2 Cache Throughput               %             1.45
    SM Active Cycles              cycle 1,488,460,573.71
    Compute (SM) Throughput           %             1.18
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          540,856
    Total DRAM Elapsed Cycles        cycle   39,708,917,760
    Average L1 Active Cycles         cycle 1,488,460,573.71
    Total L1 Elapsed Cycles          cycle   57,053,353,568
    Average L2 Active Cycles         cycle   498,463,026.38
    Total L2 Elapsed Cycles          cycle   36,067,378,320
    Average SM Active Cycles         cycle 1,488,460,573.71
    Total SM Elapsed Cycles          cycle   57,053,353,568
    Average SMSP Active Cycles       cycle   743,952,773.86
    Total SMSP Elapsed Cycles        cycle  228,213,414,272
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.53%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.59% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.53%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.80% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.53%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.59% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortColumns(float *, int, int, int *) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,370,710,714
    Memory Throughput                 %             1.83
    DRAM Throughput                   %             0.01
    Duration                          s             1.23
    L1/TEX Cache Throughput           %             2.93
    L2 Cache Throughput               %             1.46
    SM Active Cycles              cycle 1,489,137,824.42
    Compute (SM) Throughput           %             1.17
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          534,712
    Total DRAM Elapsed Cycles        cycle   39,477,297,152
    Average L1 Active Cycles         cycle 1,489,137,824.42
    Total L1 Elapsed Cycles          cycle   57,226,866,704
    Average L2 Active Cycles         cycle   496,919,526.56
    Total L2 Elapsed Cycles          cycle   35,856,999,328
    Average SM Active Cycles         cycle 1,489,137,824.42
    Total SM Elapsed Cycles          cycle   57,226,866,704
    Average SMSP Active Cycles       cycle   743,905,224.04
    Total SMSP Elapsed Cycles        cycle  228,907,466,816
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.46%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.56% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.47%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.81% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.46%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.56% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  sortColumns(float *, int, int, int *) (16, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                  Ghz             7.99
    SM Frequency                    Ghz             1.92
    Elapsed Cycles                cycle    2,382,051,208
    Memory Throughput                 %             1.84
    DRAM Throughput                   %             0.01
    Duration                          s             1.24
    L1/TEX Cache Throughput           %             2.93
    L2 Cache Throughput               %             1.46
    SM Active Cycles              cycle 1,488,543,489.17
    Compute (SM) Throughput           %             1.18
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     16
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 24             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         4.17
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (4.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ----------------
    Metric Name                Metric Unit     Metric Value
    -------------------------- ----------- ----------------
    Average DRAM Active Cycles       cycle          540,352
    Total DRAM Elapsed Cycles        cycle   39,666,141,184
    Average L1 Active Cycles         cycle 1,488,543,489.17
    Total L1 Elapsed Cycles          cycle   56,916,330,392
    Average L2 Active Cycles         cycle   503,297,326.62
    Total L2 Elapsed Cycles          cycle   36,008,648,752
    Average SM Active Cycles         cycle 1,488,543,489.17
    Total SM Elapsed Cycles          cycle   56,916,330,392
    Average SMSP Active Cycles       cycle   744,323,979.20
    Total SMSP Elapsed Cycles        cycle  227,665,321,568
    -------------------------- ----------- ----------------

    OPT   Est. Speedup: 23.52%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 37.47% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.59%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 68.79% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.52%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.47% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

